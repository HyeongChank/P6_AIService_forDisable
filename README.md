# P6_EmpCont
+ 고용노동부 공공데이터 활용 공모전
+ 장애인 고용 관련 아이디어 제출

아이디어명
아이디어 구상 및 제안 배경
아이디어의 고용노동 데이터 호라용 방안
-아이디어에 어떤 고용노동 데이터가 어떻게 적절히 활용되었는지 작성
-활용하고자 하는 타 기관 또는 민간 데이터명 작성
-공공데이터의 획득의 지속성 및 활용범위, 가공 등의 가능성에 대해서 작성

아이디어의 상세 설명(실용성)
-아이디어에 대한 상세 설명과 작동방법(주요기능 미 이미지, 핵심기술)을 작성

아이디어의 독창성
- 아이디어만의 창의적, 독창적 기술, 서비스 등에 대해서 구체적으로 작성

아이디어 결과물의 성과 및 기대효과
-아이디어 추진 이전와 이후에 실질적으로 국민 생활에 도움이 되는지 작성


jupyter nbconvert --to html notebook/CNN_prediction.ipynb
>>   다음 줄에 >> 가 나와야 함
jupyter nbconvert --to html notebook/disabled_findjob.ipynb
>>

GPT-2와 같은 언어 모델을 학습시키는 과정은 일반적으로 다음과 같은 단계를 포함합니다.

데이터 준비: 모델을 학습시키기 위해서는 대량의 텍스트 데이터가 필요합니다. 이 데이터는 모델이 패턴을 학습할 수 있도록 충분한 정보를 제공해야 합니다. 학습 데이터는 텍스트 파일 형태로 준비되며, 한 줄이 하나의 문서 또는 문장으로 구성될 수 있습니다.

토큰화: 텍스트 데이터를 모델이 이해할 수 있는 형태로 변환해야 합니다. 이 과정에서는 텍스트를 토큰(token)이라는 작은 단위로 분리하며, 토큰은 단어나 부분 단어일 수 있습니다. 이 과정은 토크나이저(tokenizer)를 사용하여 수행합니다.

모델 학습: 토큰화된 데이터를 모델에 입력으로 제공하고, 모델이 이를 바탕으로 패턴을 학습하도록 합니다. 모델은 입력 토큰들을 보고 다음에 올 토큰을 예측하는 방식으로 학습합니다.

Hugging Face의 Transformers 라이브러리는 이러한 학습 과정을 지원하며, Trainer 클래스를 사용하면 모델 학습을 쉽게 수행할 수 있습니다. 이 라이브러리의 문서에서는 다양한 학습 설정과 방법에 대해 상세하게 설명하고 있습니다.